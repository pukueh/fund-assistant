"""Celery åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦å™¨

ä½¿ç”¨ Celery + Redis å®ç°åˆ†å¸ƒå¼å®šæ—¶ä»»åŠ¡ï¼š
- æ¯æ—¥ 17:00 æŠ“å–åŸºé‡‘å‡€å€¼
- æ¯å‘¨æ—¥ æŠ“å–åŸºé‡‘æŒä»“é…ç½®  
- æ¯æœˆåˆ æ›´æ–°é‡åŒ–æŒ‡æ ‡
"""

import os
from datetime import datetime
from typing import List, Optional

# Celery æ˜¯å¯é€‰ä¾èµ–
try:
    from celery import Celery
    from celery.schedules import crontab
    CELERY_AVAILABLE = True
except ImportError:
    CELERY_AVAILABLE = False
    Celery = None
    crontab = None

# Celery é…ç½®
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")

# åˆ›å»º Celery åº”ç”¨ï¼ˆä»…åœ¨ Celery å¯ç”¨æ—¶ï¼‰
celery_app = None
if CELERY_AVAILABLE:
    celery_app = Celery(
        "fund_assistant",
        broker=REDIS_URL,
        backend=REDIS_URL,
        include=[
            "data_ingestion.tasks"
        ]
    )
    
    # Celery é…ç½®
    celery_app.conf.update(
        # æ—¶åŒº
        timezone="Asia/Shanghai",
        enable_utc=False,
        
        # ä»»åŠ¡åºåˆ—åŒ–
        task_serializer="json",
        result_serializer="json",
        accept_content=["json"],
        
        # ä»»åŠ¡æ‰§è¡Œ
        task_acks_late=True,
        task_reject_on_worker_lost=True,
        worker_prefetch_multiplier=1,
        
        # ç»“æœè¿‡æœŸ
        result_expires=3600,
        
        # å®šæ—¶ä»»åŠ¡
        beat_schedule={
            # æ¯æ—¥ 17:00 é‡‡é›†å‡€å€¼
            "collect-nav-daily": {
                "task": "data_ingestion.tasks.collect_all_nav",
                "schedule": crontab(hour=17, minute=0),
                "options": {"queue": "data_ingestion"}
            },
            # æ¯å‘¨æ—¥ 10:00 é‡‡é›†æŒä»“
            "collect-holdings-weekly": {
                "task": "data_ingestion.tasks.collect_all_holdings",
                "schedule": crontab(hour=10, minute=0, day_of_week=0),
                "options": {"queue": "data_ingestion"}
            },
            # æ¯æœˆ1æ—¥ 10:00 æ›´æ–°æŒ‡æ ‡
            "update-metrics-monthly": {
                "task": "data_ingestion.tasks.update_all_metrics",
                "schedule": crontab(hour=10, minute=0, day_of_month=1),
                "options": {"queue": "data_ingestion"}
            },
            # æ¯5åˆ†é’Ÿæ›´æ–°å®æ—¶ä¼°å€¼ï¼ˆäº¤æ˜“æ—¶é—´ï¼‰
            "update-realtime-nav": {
                "task": "data_ingestion.tasks.update_realtime_nav",
                "schedule": crontab(minute="*/5", hour="9-15", day_of_week="1-5"),
                "options": {"queue": "realtime"}
            }
        },
        
        # ä»»åŠ¡è·¯ç”±
        task_routes={
            "data_ingestion.tasks.collect_*": {"queue": "data_ingestion"},
            "data_ingestion.tasks.update_realtime_*": {"queue": "realtime"}
        }
    )


def start_scheduler():
    """å¯åŠ¨è°ƒåº¦å™¨ï¼ˆå¼€å‘æ¨¡å¼ï¼‰
    
    ç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨:
    celery -A data_ingestion.scheduler beat --loglevel=info
    celery -A data_ingestion.scheduler worker --loglevel=info -Q data_ingestion,realtime
    """
    print("ğŸ“… Celery Beat è°ƒåº¦å™¨é…ç½®å®Œæˆ")
    print("=" * 50)
    print("å®šæ—¶ä»»åŠ¡:")
    for name, config in celery_app.conf.beat_schedule.items():
        print(f"  - {name}: {config['schedule']}")
    print("=" * 50)
    print("\nå¯åŠ¨å‘½ä»¤:")
    print("  celery -A data_ingestion.scheduler beat --loglevel=info")
    print("  celery -A data_ingestion.scheduler worker --loglevel=info -Q data_ingestion,realtime")


# ç®€æ˜“çš„ APScheduler å¤‡ç”¨æ–¹æ¡ˆï¼ˆä¸éœ€è¦ Redisï¼‰
class SimpleScheduler:
    """ç®€æ˜“è°ƒåº¦å™¨ï¼ˆæ—  Redis ä¾èµ–ï¼‰
    
    ç”¨äºå¼€å‘å’Œå•æœºéƒ¨ç½²åœºæ™¯
    """
    
    def __init__(self):
        self._jobs = {}
        self._running = False
        self._scheduler = None
    
    def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        try:
            from apscheduler.schedulers.background import BackgroundScheduler
            from apscheduler.triggers.cron import CronTrigger
            
            self._scheduler = BackgroundScheduler(timezone="Asia/Shanghai")
            
            # æ·»åŠ ä»»åŠ¡
            self._scheduler.add_job(
                self._collect_nav_task,
                CronTrigger(hour=17, minute=0),
                id="collect_nav_daily",
                name="æ¯æ—¥å‡€å€¼é‡‡é›†"
            )
            
            self._scheduler.add_job(
                self._collect_holdings_task,
                CronTrigger(hour=10, minute=0, day_of_week=6),
                id="collect_holdings_weekly",
                name="æ¯å‘¨æŒä»“é‡‡é›†"
            )
            
            self._scheduler.add_job(
                self._update_metrics_task,
                CronTrigger(hour=10, minute=0, day=1),
                id="update_metrics_monthly",
                name="æ¯æœˆæŒ‡æ ‡æ›´æ–°"
            )
            
            self._scheduler.start()
            self._running = True
            print("ğŸ“… APScheduler è°ƒåº¦å™¨å·²å¯åŠ¨")
            
        except ImportError:
            print("âš ï¸ APScheduler æœªå®‰è£…ï¼Œè¯·ä½¿ç”¨ Celery æˆ–æ‰‹åŠ¨æ‰§è¡Œä»»åŠ¡")
    
    def stop(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        if self._scheduler:
            self._scheduler.shutdown()
            self._running = False
            print("ğŸ“… è°ƒåº¦å™¨å·²åœæ­¢")
    
    def _collect_nav_task(self):
        """å‡€å€¼é‡‡é›†ä»»åŠ¡"""
        from .collectors import NavCollector
        collector = NavCollector()
        collector.collect_all()
    
    def _collect_holdings_task(self):
        """æŒä»“é‡‡é›†ä»»åŠ¡"""
        from .collectors import EventsCollector
        collector = EventsCollector()
        collector.collect_all()
    
    def _update_metrics_task(self):
        """æŒ‡æ ‡æ›´æ–°ä»»åŠ¡"""
        from .collectors import MetricsCollector
        collector = MetricsCollector()
        collector.update_all()


# å¯¼å‡ºç®€æ˜“è°ƒåº¦å™¨å®ä¾‹
simple_scheduler = SimpleScheduler()
